{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PDF and CDF\n",
    "Probabality denisty functions are the way we talk about the probabality a random variable. For a pdf f we say that f(x) is defined as the the probablity that this random variable in question takes value x. Formally we might say that f(x)=Pr(X=x) where X is the variable in question. \n",
    "\n",
    "1. If X represents the roll of a fair 6 sided die then the $f(1)=f(2)=...=f(6) = \\frac{1}{6}$ and for fullness you can define $f(x) = 0$ for any other value of x.  \n",
    "\n",
    "This definition is clear for discrete variables but for continous variables the definition is needs more care. \n",
    "\n",
    "The exponential random variable is defined by $f(x) = \\lambda e^{-\\lambda x}$ where $\\lambda$ is a constant. We will talk more about why we would care about such a definition later. What does it mean to say here $f(x) = 1/2$ or any other value? It does not mean the probabality that X=x is 1/2. It means instead that the probablity density at this point is 1/2. \n",
    "\n",
    "If  $f(x) =  e^{-x}$ then $f(1) = e^{-1} \\approx 0.36$. But $Pr(X=1)=0$. \n",
    "\n",
    "One meaningful way to can use f is to say that the $Pr(X \\in (1 -\\epsilon, 1+\\epsilon))= $2e^{-1}\\epsilon $ for some very small $\\epsilon$.  Note generally this is only a valid approximation for small $\\epsilon$ \n",
    "\n",
    "\n",
    "## CDFs \n",
    "Another meaningful way to use the density is the talk about the cumlative probabality of of getting a value at or below x. The CDF is $F(x) = Pr(X \\leq x)$ and notational we do culturally use capital letters for cdfs and lowercase for pdfs. Note for all valid distrubtions $F(\\infty) = 1$ this means that something must happen.  \n",
    "For discrete random variables the cdf if $F(x) = \\sum_{=-\\infty}^{\\infty} f(n)$ and for continous random variables $F(x) = \\int_{-\\infty}^{x} f(x)$.\n",
    "\n",
    "### Examples. \n",
    "\n",
    "## Joint Probabality \n",
    "We will also care about the probabality of events happening jointly ex. the probabality it rains heavily on my birthday. For two random variables X and Y we call $Pr(X=x, Y=y)$ the joint probabality distrubtion. \n",
    "\n",
    "### Example \n",
    "\n",
    "In general, the joint probabality distribution of two variables is not multiplicative that is  $Pr(X=x, Y=y) \\neq Pr(X=x) Pr(Y=y)$. But when this property does hold we say X and Y are independent. \n",
    "\n",
    "### Examples \n",
    "\n",
    "## Conditional Probabality \n",
    "Conditional probablity is asking what is the probabality of an event happens knowing something else has happened. For instance, we might want to know the probabality that this quarter we get \\$ x sales knowing that last quarter the company made \\$ y in sales. \n",
    "\n",
    "Formerly, we write this by $Pr(X | Y)$. \n",
    "### Example \n",
    "\n",
    "One additional way we can evaluate the conditional probabality is by taking $Pr(X|Y) = \\frac{Pr(X,Y)}{Pr(Y)}$. The way to think about this is chance X and Y both happen normalized by the chance that Y happens. \n",
    "### Example \n",
    "\n",
    "If X and Y are independent then $Pr(X|Y) = \\frac{Pr(X,Y)}{Pr(Y)} = \\frac{Pr(X)Pr(Y)}{Pr(Y)} = Pr(X)$. \n",
    "\n",
    "## Expected Value \n",
    "The expected value is the mean outcome we can expect. If you ran the same experiment many times and took the average the value you get will be close to the expected value.  The expected value of a discrete variable X is $\\mathbb{E}[X] = \\sum_{n=-\\infty}^{\\infty} Pr(X=n) * n = \\sum_{n=-\\infty}^{\\infty} f(n) * n$ and for a continous variable $\\mathbb{E}[X] = \\infty_{x=-\\infty}^{\\infty} Pr(X=x) x = \\infty_{x=-\\infty}^{\\infty} f(x) x$.\n",
    "\n",
    "### Examples of finding CDFs \n",
    "\n",
    "## Linearity of Expectation\n",
    "There are several properties of expected value but the most important is linearity of expectation. This says that if I have two variables X and Y then $E[X+Y] = E[X] + E[Y]$ and that $E[aX] = aE[X]$ where a is a constant. This is a property you really need to know for this class and can be counterintuitive when you consider this works for events that are related. \n",
    "\n",
    "### Examples\n",
    "1. Consider a fair die. Let $X$ be a random variable such that $X=1$ is the die rolls 1, 2, or 3 and zero otherwise and let $Y$ be random variable such that $Y=1$ if the die rolls 2 or 4 and 0 otherwise. What is the expected value of $X+Y$. \n",
    "\n",
    "answer: $E[X+Y] = E[X] + E[Y]$ and we calculate each seperately. $E[X] = \\frac{1}{2}$ and $E[Y]=\\frac{1}{3}$ so the final answer is $\\frac{5}{6}$. \n",
    "\n",
    "## When do we have linearity of variance \n",
    "While we do have linearity of expectation, it is not generally not the case that $Var(X+Y) = Var(X) + Var(Y)$. Instead\n",
    "\\begin{align}\n",
    "    Var(X+Y) &= E[(X+Y - E[X+Y])^2] \\\\ \n",
    "    &= E[X^2 + Y^2 + 2XY - 2(X+Y)E[X+Y] + E[X+Y]^2] \\\\ \n",
    "    &= E[X^2 + 2XY + Y^2] - 2E[X+Y]^2 + E[X+Y]^2 \\\\ \n",
    "    &= E[X^2 + 2XY + Y^2] - E[X+Y]^2 \\\\  \n",
    "    &= E[X^2] + 2 E[XY] + E[Y^2] - (E[X]^2 + E[X]E[Y] + E[Y]^2) \\\\\n",
    "    &= Var[X] + Var[Y] + 2 (E[XY] - E[X]E[Y])\n",
    "\\end{align}\n",
    "\n",
    "We call $E[XY] - E[X]E[Y]$ the covariance of X and Y. Notice that if X and Y are independent then \n",
    "\\begin{align}\n",
    "    E[XY] &= \\int_{0}^\\infty z Pr(X*Y = z ) dz \\\\ \n",
    "    &= \\int_{0}^\\infty \\int_{0}^\\infty a * b Pr(X = a, Y=b) da db \\\\\n",
    "    &= \\int_{0}^\\infty \\int_{0}^\\infty a * b Pr(X = a) Pr(Y=b) da db \\\\\n",
    "    &= \\int_{0}^\\infty b Pr(Y=b) \\int_{0}^\\infty a Pr(X=a) da db \\\\ \n",
    "    &= \\int_{0}^\\infty b Pr(Y=b) E[X] db \\\\ \n",
    "    &= E[Y] E[X]\n",
    "\\end{align}\n",
    "\n",
    "So in summary, $Var(X+Y) = Var(X) + Var(Y)$ only when X and Y are have a covariance of zero. One way but not the only for X and Y to have a covariance of 0 is for X and Y to be independent. \n",
    "\n",
    "### Counter example: $COV(X,Y) =0 \\to X \\bot Y$ is not true \n",
    "Suppose we pick a random number uniformly from -1 to 1. Call the result X. Then $E[X] = \\int_{-1}^1 \\frac{x}{2} dx = 0$. Let Y be the $X^2$. Clearly, X and Y are not indendent because $E[Y= y| X=x] = \\begin{cases} 1 & \\text{if } x = \\pm \\sqrt{y} \\\\ 0 & \\text{Else} \\end{cases}$. \n",
    "\n",
    "But then \n",
    "\\begin{align}\n",
    "    E[Y] &= \\int_{0}^1 x^3 dx \\\\ \n",
    "    &= \\frac{1}{4}\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "\\begin{align}\n",
    "    E[XY] &= \\frac{1}{2} \\int_{-1}^1 x * x^2 dx \\\\ \n",
    "    &= \\frac{1}{2 * 8} x^4|_{-1}^1 \\\\ \n",
    "    &= 0 \n",
    "\\end{align}\n",
    "\n",
    "So then $COV(X,Y) = E[XY] - E[X]E[Y] = 0 - 0 = 0$\n",
    "\n",
    "\n",
    "## Advanced expected value  \n",
    "\n",
    "### Long highway problem \n",
    "Consider N cars in a random order on a very long single lane highway. The driver has a preferred speed they would like to go at if able. They would \n",
    "be unable to go that speed only if they are stuck behind a car going slower than them. As the highway has only one lane they will be stuck behind \n",
    "this slow driver. Eventually \"clusters\" of cars will form where a cluster is one slow driver and everyone stuck behind them so would like to go faster\n",
    " but is blocked by this slow driver. The preferred speed of drivers is chosen uniformly randomly from 0 to 100 mph.\n",
    "\n",
    "*Example* Suppose  Alice wants to drive at 50 mph, Bob at 40 mph, and Claire at 60 mph and they are ordered by Alice first, Bob second, Claire third.\n",
    " Then two clusters will eventually form. The first is just Alice and the second is Bob and Claire as Claire is stuck behind Bob. \n",
    "\n",
    "If there are N cars what is the expected number of clusters? \n",
    "\n",
    "Answer: Let $X_i = 1$ if $X_i$ is the first car in a cluster and zero otherwise. In order for i to be the start of a cluster it want to travel slower than every other car in the front of it. This is actually easy to compute. If there are i items randomly shuffled the chance that one in position i is the smallest is $\\frac{1}{i}$. But the number of clusters is the equivalently the number of lead cars. Let $X$ be the number of clusters then \n",
    "\\begin{align}\n",
    "    E[X] = E[\\sum_{i=1}^{n} X_i] \\\\ \n",
    "    &= \\sum_{i=1}^{n} E[X_i] \\\\ \n",
    "    &= \\sum_{i=1}^{n} \\frac{1}{i} \n",
    "\\end{align}\n",
    "\n",
    "If you recall your calculus you will recognize that this is the nth harmonic number $H_n$ and that this growth with rate $O(log(n))$. \n",
    "\n",
    "\n",
    "### Probabalistic method \n",
    "Let $v_1, v_2, \\cdots, v_n$ be a collection of vectors with $\\| v_i\\| = 1$. We can flip the sign of each $v_i$. Let $s_i$ be the sign choosen that is $s_i = \\pm 1$. The question is how small we can make the total $\\| \\sum_{i=1}^n s_i v_i\\|$. Show that there exists a choice of signs with $\\| \\sum_{i=1}^n s_i v_i\\| \\leq \\sqrt{n}$ for all n and collection of vectors. \n",
    "\n",
    "Choose the sign of each $s_i$ randomly. Then let $X = \\| \\sum_{i=1}^n s_i v_i\\|^2$ for this choice. We have then \n",
    "\\begin{align}\n",
    "    X = \\sum_{i=1}^n \\sum_{j=1}^n s_i s_j v_i^T v_j \\\\ \n",
    "    E[X] &= E[\\sum_{i=1}^n \\sum_{j=1}^n s_i s_j v_i^T v_j] \\\\ \n",
    "    &= \\sum_{i=1}^n \\sum_{j=1}^n E[s_i s_j v_i^T v_j]  \\\\ \n",
    "    &= \\sum_{i=1}^n \\sum_{j=1}^n E[s_i s_j] v_i^T v_j \\\\ \n",
    "\\end{align}\n",
    "If $i=j$ then $s_i s_j = 1$ and $v_i^T v_j = 1$ by assumption. If $i\\neq j$ however $E[s_i s_j] = $E[s_i] E[s_j]$ because $s_i$ and $s_j$ were choosen independently. But $E[s_i] = 0.5 * (-1) + 0.5 * (+1) = 0$ and same will go for $E[s_j]$. Therefore the total expected value of X is \n",
    "\\begin{align}\n",
    "    E[X] &= \\sum_{i=1}^n \\sum_{j=1}^n E[s_i s_j] v_i^T v_j \\\\  \n",
    "    &= \\sum_{i=1}^n \\sum_{\\substack{j=1\\\\j\\neq i}}^n E[s_i s_j] v_i^T v_j \\\\\n",
    "    &+ \\sum_{i=1}^n E[s_i s_i] v_i^T v_j \\\\ \n",
    "    &= 0 + n \\\\ \n",
    "    &= n \n",
    "\\end{align}\n",
    "\n",
    "Since $\\mathbb{E}[X] = n$ there must be at least one choice of signs that achieves at least a norm squared of n. \n",
    "\n",
    "## Variance \n",
    "\n",
    "## Common distributions\n",
    "\n",
    "A. Bernouli random variable \n",
    "A bernouli random variable is essentially a weight coin. It is 1 with probabality p and 0 with probabality 1-p. So $\\mathbb{E}[X] = $p$. \n",
    "\n",
    "*Variance of a bernouli random variable* \n",
    "$E[X]^2 = p^2$. $E[X^2] = 1^2 * p + 0^2 * (1-p) = p$ so Var = p^2 - p \n",
    "\n",
    "\n",
    "B. Binomial \n",
    "A binomial variable is just the sum of running several independent bernoili random variable experiments. Precisely it is the probabality of getting k successes out of n coin flips. The probabality we get k successes is $(n choose k) * p^k * (1-p)^(n-k)$. \n",
    "\n",
    "*Mean and variance of a binomial variable* \n",
    "\\begin{align}\n",
    "    E[X] &= \\sum_{k=0}^n E[X_i] \\\\ \n",
    "    &= n * p \n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    Var[X] &= \\sum_{k=0}^n  Var[X_i] \\\\ \n",
    "    &= n * p * (1-p)\n",
    "\\end{align}\n",
    "\n",
    "C. geometric\n",
    "A geometric variable is how many coin flips we have to do until we get our first success. So if heads is a success then we might flip TTH in which case it took three flips. \n",
    "*Mean 1*\n",
    "\\begin{align}\n",
    "    E[X] &= \\sum_{k=1}^\\infty k p * (1-p)^{k-1} \n",
    "\\end{align}\n",
    "One way to solve this is with the following trick. Let $f(q) = \\sum_{k=0}^\\infty q^{k} = \\frac{1}{1-q}$. Then $f'(q) = \\sum_{k=0}^\\infty  k q^{k-1} = \\sum_{k=1}^\\infty k q^{k-1}$. But $f'(q) = \\frac{1}{(1-q)^2}$.  Therefore $E[X] = \\frac{p}{p^2} = \\frac{1}{p}$. \n",
    "\n",
    "You could use this sort of reasoning to figure out the variance by calculating $E[X(X-1)]$ as a first step but for now I will just tell you that the variance is $\\frac{1-p}{p^2}$. \n",
    "*Mean 2* \n",
    "\\begin{align} \n",
    "    E[X] &= p * 1 + (1-p) *( 1 + E[X])\n",
    "    p E[X] &= 1 \\\\ \n",
    "    E[X] &= 1/p \n",
    "\\end{align}\n",
    "\n",
    "#### Advanced Time until a sequence \n",
    "Suppose we have a coin that is heads with Pr(H) = 1/3 and tails with Pr(2/3). What is the expected number of flips needed until we see the sequence HHT. Formulate symbolically the system of equations to sovle but you don't need to actually solve. \n",
    "\n",
    "\\begin{align}\n",
    "    E[X] &= p*(E[X_{H}] + 1) + (1-p)*(E[X] + 1) \\\\ \n",
    "    E[X_H] &= p*(E[X_{HH}] + 1) + (1-p)*(E[X]+1) \\\\ \n",
    "    E[X_{HH}] &= p*(E[X_{HH}]+1) + (1-p)*1 \n",
    "\\end{align}\n",
    "\n",
    "d. Uniform\n",
    "\n",
    "A uniform variable takign any continous value from parameters (a,b) with uniform probabality. So the probabality denisty of seeing a particular value is $\\frac{1}{b-a}. \n",
    "*Mean and Variance* \n",
    "\\begin{align}\n",
    "    E[X] &= \\int_{a}^b \\frac{x}{b-a} dx  \\\\ \n",
    "    &= \\frac{1}{2 (b-a)} x^2 |_a^b \\\\ \n",
    "    &= \\frac{b^2 - a^2}{2 (b-a)} \\\\ \n",
    "    &= \\frac{(b-a)(b+a)}{2 (b-a)} \\\\ \n",
    "    \\frac{b+a}{2}\n",
    "\\end{align}\n",
    "This should also make intuitive sense. \n",
    "\n",
    "Variance \n",
    "\\begin{align}\n",
    "    E[X^2] &= \\int_{a}^b \\frac{x^2}{b-a} dx \\\\ \n",
    "    &= \\frac{1}{3(b-a)} x^3|_a^b \\\\ \n",
    "    &= \\frac{b^3 - a^3}{3 (b-a)}\n",
    "\\end{align}\n",
    "\n",
    "So the variance is \n",
    "\\begin{align}\n",
    "    Var[X] &= E[X^2] - E[X]^2 \\\\ \n",
    "    &= \\frac{b^3 - a^3}{3 (b-a)} - \\frac{b^2 + 2ab+a^2}{4} \\\\ \n",
    "    &= \\frac{1}{12} (b-a)^2\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "e. exponential\n",
    "\n",
    "The exponential random variable in some sense is like the continous version of a geometric random variable. One property that makes this connection sensible is that for the geometric random variable if we know we have flipped 100 coins or 100,000 the probabality that the next flip is heads is the same. This property is called memoryless, the past outcomes don't affect the probabality of the next event being a heads. The way to say this mathematically is that for the the exponential random variable $Pr(T > s+t | T > s  ) = Pr(T > t)$. This is not an obvious property. For some variables knowing that a certain amount of time has passed before the event might make the event more or less likely. A car tire might have some failure rate but one we could model as increasing, a fresh tire is less likely to fail than one which has been driven for a long time. One nice thing about this disbution is the math is easy. \n",
    "\n",
    "The probabality denisty of an exponential random variable is $f(x) = \\lambda e^{-\\lambda x}$ where $\\lambda > 0$ is a parameter. We often call $\\lambda$ the rate. \n",
    "\n",
    "*Mean* \n",
    "\\begin{align}\n",
    "    E[X] &= \\int_{0}^\\infty x * \\lambda * e^{-\\lambda x} dx \\\\ \n",
    "    &= \\frac{1}{\\lambda} \\int_{0}^\\infty y * e^{-y} dy \\\\ \n",
    "    &= \\frac{1}{\\lambda} \\int_{0}^\\infty y * e^{-y} dy \\\\ \n",
    "    u &= y \\\\\n",
    "    dv = e^{-y } \\\\ \n",
    "    &= \\frac{1}{\\lambda} ( -y e^{-y} - \\int -e^{-y} dy ) \\\\ \n",
    "    &= \\frac{1}{\\lambda} ( -y e^{-y}|_0^\\infty - e^{-y}|_0^\\infty  ) \\\\ \n",
    "    &= \\frac{1}{\\lambda}  \n",
    "\\end{align}\n",
    "\n",
    "*CDF* \n",
    "\\begin{align}\n",
    "    F(x) &= \\int_{0}^x \\lambda e^{-\\lambda x} \\\\ \n",
    "    &= -e^{\\lambda x }|_0^x \\\\ \n",
    "    &= 1 - e^{-\\lambda x}\n",
    "\\end{align}\n",
    "\n",
    "*Memoryless propery* \n",
    "\\begin{align}\n",
    "    Pr(T > t +s | T > t) &=  \\frac{Pr(T > t+ s)}{Pr(T > t)} \\\\\n",
    "     &= \\frac{e^{-\\lambda (t + s)}  }{e^{-\\lambda t}} \\\\ \n",
    "     &= e^{-\\lambda s} \n",
    "\\end{align}\n",
    "\n",
    "f. Poisson\n",
    "\n",
    "A poisson process is like the continous version of a binomial distrubtion. It is the probability we get k successes in a given time frame. The analogy is are we are asking about how many exponential variables(bernoulli) go off in a time span(number of coin flips).  \n",
    "The pdf of a Poisson random variable $Pr(X=k \\text{ in t time}) = \\frac{(-\\lambda t)^k e^{-\\lambda t}}{k!}$. We will explain the rationale of this more later. \n",
    "\n",
    "This distribution is important because it is used extensively in modeling queueing systems. For example, we might model customers arrival as \n",
    "\n",
    "g. Normal\n",
    "\n",
    "\n",
    "## Binomial Vs Poisson \n",
    "We will show how binomial and poisson are related. Consider the events that happen in T units of time. Let's divide up this interval into n even slices. For each slice we will flip a coin with probabality $p = \\frac{\\lambda *T }{n}$. Then for a binomial random variable the chance of having k successes is $\\dbinom{n}{k} (\\frac{\\lambda T}{n}) ^k (1-\\frac{\\lambda T}{n})^k$. We can associate with these terms  a polynomial where the cofficient in front of $x^k$ is the probabality of having k successes. This is called a generating function. \n",
    "\n",
    "That is we define $f(x) = \\sum_{k=0}^n P(\\text{k successes}) x^k = \\sum_{k=0}^n \\dbinom{n}{k} (\\frac{\\lambda T  x}{n})^k  (1- \\frac{\\lambda T }{n})^{n-k}$. This by the binomial series is $( 1 - \\frac{\\lambda T}{n} + \\frac{\\lambda T x}{n})^n$. This reduces to $(1 + \\frac{ \\lambda T ( x - 1) }{n})^n$ which for large n reduces to $e^{\\lambda T (x-1)  }$. But this itself can be expanded as $\\sum_{k=0}^\\infty e^{-\\lambda T} * \\frac{(\\lambda T x)^k }{k!}$ which is a poisson distrubition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
